{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@authors\n",
    "* Arseniy Ashuha, you can text me ```ars.ashuha@gmail.com```,\n",
    "* Based on https://github.com/ebenolson/pydata2015\n",
    "\n",
    "<h1 align=\"center\"> Part II: Attention mechanism @ Image Captioning </h1> \n",
    "\n",
    "<img src=\"https://s2.postimg.org/pq18f5t7t/deepbb.png\" width=480>\n",
    "\n",
    "In this seminar you'll be going through the image captioning pipeline.\n",
    "\n",
    "To begin with, let us download the dataset of image features from a pre-trained GoogleNet (see instructions in chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "import numpy as np\n",
    "\n",
    "captions = np.load(\"../download/train-data-captions.npy\")\n",
    "img_codes = np.load(\"../download/train-data-svdfeatures.npy\").astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "each image code is a 6x6 feature matrix from GoogleNet: (82783, 128, 6, 6)\n",
      "[-19.53911972   3.23637891   2.08816719   0.66493636  -2.9185071\n",
      "   1.82758021  -1.3254329   -0.53197509  -3.15473676  -1.8739953 ]\n",
      "\n",
      "\n",
      "\n",
      "for each image there are 5-7 descriptions, e.g.:\n",
      "\n",
      "People shopping in an open market for vegetables.\n",
      "An open market full of people and piles of vegetables.\n",
      "People are shopping at an open air produce market.\n",
      "Large piles of carrots and potatoes at a crowded outdoor market.\n",
      "People shop for vegetables like carrots and potatoes at an open air market.\n"
     ]
    }
   ],
   "source": [
    "print (\"each image code is a 6x6 feature matrix from GoogleNet:\", img_codes.shape)\n",
    "print (img_codes[0,:10,0,0])\n",
    "print ('\\n\\n')\n",
    "print (\"for each image there are 5-7 descriptions, e.g.:\\n\")\n",
    "print ('\\n'.join(captions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split descriptions into tokens\n",
    "for img_i in range(len(captions)):\n",
    "    for caption_i in range(len(captions[img_i])):\n",
    "        sentence = captions[img_i][caption_i] \n",
    "        captions[img_i][caption_i] = [\"#START#\"]+sentence.split(' ')+[\"#END#\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Vocabulary\n",
    "from collections import Counter\n",
    "word_counts = Counter()\n",
    "for img_captions in captions:\n",
    "    for caption in img_captions:\n",
    "        word_counts.update(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab  = ['#UNK#', '#START#', '#END#']\n",
    "vocab += [k for k, v in word_counts.items() if v >= 5]\n",
    "vocab = list(set(vocab))\n",
    "n_tokens = len(vocab)\n",
    "\n",
    "assert 12000 <= n_tokens <= 15000\n",
    "\n",
    "word_to_index = {w: i for i, w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use this function to convert sentences into a network-readible matrix of token indices.\n",
    "\n",
    "When given several sentences of different length, it pads them with -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD_ix = -1\n",
    "UNK_ix = vocab.index('#UNK#')\n",
    "START_ix = vocab.index(\"#START#\")\n",
    "END_ix = vocab.index(\"#END#\")\n",
    "\n",
    "#good old as_matrix for the third time\n",
    "def as_matrix(sequences,max_len=None):\n",
    "    max_len = max_len or max(map(len,sequences))\n",
    "    \n",
    "    matrix = np.zeros((len(sequences),max_len),dtype='int32')+PAD_ix\n",
    "    for i,seq in enumerate(sequences):\n",
    "        row_ix = [word_to_index.get(word,UNK_ix) for word in seq[:max_len]]\n",
    "        matrix[i,:len(row_ix)] = row_ix\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "def to_string(tokens_ix):\n",
    "    assert len(np.shape(tokens_ix))==1,\"to_string works on one sequence at a time\"\n",
    "    tokens_ix = list(tokens_ix)[1:]\n",
    "    if END_ix in tokens_ix:\n",
    "        tokens_ix = tokens_ix[:tokens_ix.index(END_ix)]\n",
    "    return \" \".join([vocab[i] for i in tokens_ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9903,  7541,  2673,  9869,  3759,  2030,     0,   961,  1303,\n",
       "        10923,  2030, 11889, 12796,    -1,    -1,    -1],\n",
       "       [ 9903,  7541,  2673, 13076,   896,  6449,  8395, 12878,  9404,\n",
       "          167,  3140, 12796,    -1,    -1,    -1,    -1],\n",
       "       [ 9903,  7541, 10231,  2054, 13004,  3562,  9840,   961,  4424,\n",
       "         6075,  7050,  7345, 10447,   474,  2554, 12796],\n",
       "       [ 9903,  1926,   961,  5786,  4591,  9073,  2565, 10013,  2030,\n",
       "         3062,  3161,  2030,  1411,  7345,  9451, 12796],\n",
       "       [ 9903,  7541, 10795,  4960,   961,  5786,  2054, 13004,  3759,\n",
       "         2030,  2554, 12796,    -1,    -1,    -1,    -1]], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try it out on several descriptions of a random image\n",
    "as_matrix(captions[1337])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A woman standing on a  tennis court holding a racquet.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_string(as_matrix(captions[1337])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The neural network\n",
    "\n",
    "Since the image encoder CNN is already applied, the only remaining part is to write a sentence decoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano, theano.tensor as T\n",
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "from lasagne.init import Normal\n",
    "theano.config.compute_test_value = 'ignore'\n",
    "theano.config.warn_float64 = 'raise'\n",
    "\n",
    "class AttentionWeights(MergeLayer):\n",
    "    def __init__(self, encoder_seq, attn_query, num_units):\n",
    "        MergeLayer.__init__(self, [encoder_seq, attn_query])\n",
    "        \n",
    "        enc_units = encoder_seq.output_shape[2]\n",
    "        dec_units = attn_query.output_shape[1]\n",
    "        \n",
    "        self.W_enc = self.add_param(Normal(), (enc_units, num_units), name='enc_to_hid')\n",
    "        self.W_query = self.add_param(Normal(), (dec_units, num_units), name='dec_to_hid')\n",
    "        self.W_out = self.add_param(Normal(), (num_units, ),name='hid_to_logit')\n",
    "    \n",
    "    def get_output_for(self, inputs):\n",
    "        # the encoder_sequence shape = [batch, time,units]\n",
    "        # the query shapeshape  = [batch, units]\n",
    "        encoder_sequence, query = inputs\n",
    "        \n",
    "        # Hidden layer activations, shape [batch,seq_len,hid_units]\n",
    "        \n",
    "        query_to_hid = query.dot(self.W_query)[:,None,:]\n",
    "        \n",
    "        # enc_to_hid = <Your code: contributon from encoder to hid, shape:[batch,time,units]>\n",
    "        enc_to_hid = encoder_sequence.astype('float32').dot(self.W_enc)\n",
    "        \n",
    "        hid = T.tanh(query_to_hid+enc_to_hid)\n",
    "        \n",
    "        # Logits from hidden, [batch_size, seq_len]\n",
    "        logits = T.dot(hid, self.W_out)\n",
    "        \n",
    "        assert logits.ndim ==2, \"Logits must have shape [batch,time] and be 2-dimensional.\"\\\n",
    "                                \"Current amount of dimensions:\"+str(logits.ndim)\n",
    "        \n",
    "        attn_weights = T.nnet.softmax(logits)\n",
    "        return attn_weights\n",
    "    \n",
    "    def get_output_shape_for(self,input_shapes):\n",
    "        enc_shape,query_shape = input_shapes\n",
    "        return enc_shape[:-1]\n",
    "\n",
    "class AttentionOutput(MergeLayer):\n",
    "    def __init__(self, encoder_seq, attn_weights):\n",
    "        MergeLayer.__init__(self,[encoder_seq,attn_weights])\n",
    "    \n",
    "    def get_output_for(self,inputs):\n",
    "        # encoder_sequence shape = [batch,time,units]\n",
    "        # attn_weights shape = [batch,time]\n",
    "        encoder_sequence, attn_weights = inputs\n",
    "    \n",
    "        #Reshape attn_weights to make 'em 3-dimensional: [batch,time,1] - so you could multiply by encoder sequence\n",
    "        attn_weights = attn_weights.reshape([attn_weights.shape[0],attn_weights.shape[1],1])\n",
    "        \n",
    "        #Compute attention response by summing encoder elements with weights along time axis (axis=1)\n",
    "        #attn_output = <Compute attention response by summing encoder elements with weights along time axis (1)>\n",
    "        attn_output = (attn_weights * encoder_sequence.astype('float32')).sum(1)\n",
    "        return attn_output\n",
    "    \n",
    "    def get_output_shape_for(self,input_shapes):\n",
    "        enc_shape,query_shape = input_shapes\n",
    "        return (enc_shape[0],enc_shape[-1])\n",
    "# network shapes. \n",
    "EMBEDDING_SIZE = 128    #Change at your will\n",
    "LSTM_SIZE  = 256        #Change at your will\n",
    "ATTN_SIZE  = 256        #Change at your will\n",
    "FEATURES,HEIGHT,WIDTH = img_codes.shape[1:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a single LSTM step here. An LSTM step should\n",
    "* take previous cell/out and input\n",
    "* compute next cell/out and next token probabilities\n",
    "* use attention to work with image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentnet.resolver import ProbabilisticResolver\n",
    "from agentnet.memory import LSTMCell\n",
    "\n",
    "temperature = theano.shared(1.)\n",
    "class decoder:\n",
    "    prev_word = InputLayer((None,),name='index of previous word')\n",
    "    image_features = InputLayer((None,FEATURES,HEIGHT,WIDTH),name='img features')\n",
    "\n",
    "    prev_cell = InputLayer((None,LSTM_SIZE),name='previous LSTM cell goes here')\n",
    "    prev_out = InputLayer((None,LSTM_SIZE),name='previous LSTM output goes here')\n",
    "    \n",
    "    prev_word_emb = EmbeddingLayer(prev_word,len(vocab),EMBEDDING_SIZE)\n",
    "    \n",
    "    ###Attention part:\n",
    "    # Please implement attention part of rnn architecture\n",
    "    \n",
    "    #First we reshape image into a sequence of image vectors\n",
    "    image_features_seq = reshape(dimshuffle(image_features,[0,2,3,1]),[[0],-1,[3]])\n",
    "    \n",
    "    #Then we apply attention just as usual\n",
    "    attn_probs = AttentionWeights(image_features_seq, prev_word_emb, 32)\n",
    "    attn = AttentionOutput(image_features_seq, attn_probs)\n",
    "\n",
    "    lstm_input = concat([attn,prev_word_emb],axis=-1)\n",
    "\n",
    "    new_cell,new_out = LSTMCell(prev_cell,prev_out,lstm_input)\n",
    "    \n",
    "    \n",
    "    output_probs = DenseLayer(new_out,len(vocab),nonlinearity=T.nnet.softmax)\n",
    "\n",
    "    \n",
    "    output_probs_scaled = ExpressionLayer(output_probs,lambda p: p**temperature)\n",
    "    output_tokens = ProbabilisticResolver(output_probs_scaled,assume_normalized=False)\n",
    "    \n",
    "    \n",
    "    # recurrent state transition dict\n",
    "    # on next step, {key} becomes {value}\n",
    "    transition = {\n",
    "        new_cell:prev_cell,\n",
    "        new_out:prev_out\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "During training, we should feed our decoder RNN with reference captions from the dataset. Training then comes down to simple likelihood maximization problem.\n",
    "\n",
    "Deep learning people also know this as minimizing crossentropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs for sentences\n",
    "sentences = T.imatrix(\"[batch_size x time] of word ids\")\n",
    "l_sentences = InputLayer((None,None),sentences)\n",
    "\n",
    "# Input layer for image features\n",
    "image_vectors = T.tensor4(\"image features [batch,channels,h,w]\")\n",
    "l_image_features = InputLayer((None,FEATURES,HEIGHT,WIDTH),image_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentnet import Recurrence\n",
    "\n",
    "decoder_trainer = Recurrence(\n",
    "    input_sequences={decoder.prev_word:l_sentences},\n",
    "    input_nonsequences={decoder.image_features:l_image_features},\n",
    "    state_variables=decoder.transition,\n",
    "    tracked_outputs=[decoder.output_probs],\n",
    "    unroll_scan = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predictions and define loss\n",
    "next_token_probs = get_output(decoder_trainer[decoder.output_probs])\n",
    "\n",
    "next_token_probs = next_token_probs[:,:-1].reshape([-1,len(vocab)])\n",
    "next_tokens = sentences[:,1:].ravel()\n",
    "\n",
    "loss = T.nnet.categorical_crossentropy(next_token_probs,next_tokens)\n",
    "\n",
    "#apply mask\n",
    "mask = T.neq(next_tokens,PAD_ix)\n",
    "loss = T.sum(loss*mask.astype('float32'))/T.sum(mask.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#trainable NN weights\n",
    "weights = get_all_params(decoder_trainer,trainable=True)\n",
    "updates = lasagne.updates.adam(loss,weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile a functions for training and evaluation\n",
    "#please not that your functions must accept image features as FIRST param and sentences as second one\n",
    "train_step = theano.function([image_vectors,sentences],loss,updates=updates,allow_input_downcast=True)\n",
    "val_step   = theano.function([image_vectors,sentences],loss,allow_input_downcast=True)\n",
    "#for val_step use deterministic=True if you have any dropout/noize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "* You first have to implement a batch generator\n",
    "* Than the network will get trained the usual way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import choice\n",
    "\n",
    "def generate_batch(images,captions,batch_size,max_caption_len=None):\n",
    "    \n",
    "    #sample random numbers for image/caption indicies\n",
    "    random_image_ix = np.random.randint(0,len(images),size=batch_size)\n",
    "    \n",
    "    #get images\n",
    "    batch_images = images[random_image_ix]\n",
    "    \n",
    "    #5-7 captions for each image\n",
    "    captions_for_batch_images = captions[random_image_ix]\n",
    "    \n",
    "    #pick 1 from 5-7 captions for each image\n",
    "    batch_captions = list(map(choice,captions_for_batch_images))\n",
    "    \n",
    "    #convert to matrix\n",
    "    batch_captions_ix = as_matrix(batch_captions,max_len=max_caption_len)\n",
    "    \n",
    "    return batch_images, batch_captions_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-2.07359767, -0.97894174, -2.05806088, -1.89956403, -6.82178402,\n",
       "         2.34008193, -4.10232496, -5.30042219,  0.74239367,  3.04254794], dtype=float32),\n",
       " array([[ 9903,  7541,  2673,  3602,  2565,  9840,  7612,  6075,  3759,\n",
       "          2030,  1521,  4386,  2030,  1082,  4761, 12796],\n",
       "        [ 9903, 12151,  6826, 10060,  9315, 13113, 11145,  3352,  2030,\n",
       "          4374,   138, 12796,    -1,    -1,    -1,    -1],\n",
       "        [ 9903,  1926,  2651,   585,  4591, 10777,  3562,  3331,  8395,\n",
       "         12098, 12796,    -1,    -1,    -1,    -1,    -1]], dtype=int32))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bx,by = generate_batch(img_codes,captions,3)\n",
    "bx[0,:10,0,0],by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main loop\n",
    "* We recommend you to periodically evaluate the network using the next \"apply trained model\" block\n",
    " *  its safe to interrupt training, run a few examples and start training again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=50 #adjust me\n",
    "n_epochs=100 #adjust me\n",
    "n_batches_per_epoch = 50 #adjust me\n",
    "n_validation_batches = 5 #how many batches are used for validation after each epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:46<00:00,  1.83s/it]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train loss: 4.900748100280762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:00<00:00,  2.87s/it]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train loss: 4.772661609649658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:12<00:00,  2.31s/it]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, train loss: 4.624748344421387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:41<00:00,  2.05s/it]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, train loss: 4.4516128730773925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:57<00:00,  2.82s/it]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, train loss: 4.364750647544861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:48<00:00,  2.00s/it]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, train loss: 4.240622897148132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:45<00:00,  2.03s/it]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, train loss: 4.185291972160339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:42<00:00,  2.07s/it]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, train loss: 4.117165808677673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:45<00:00,  2.01s/it]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, train loss: 4.059547119140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [1:07:47<00:00, 1176.73s/it]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, train loss: 4.006289596557617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:16<00:00,  2.16s/it]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, train loss: 3.9884975719451905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:45<00:00,  2.24s/it]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, train loss: 3.8984531450271604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:45<00:00,  2.41s/it]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, train loss: 3.8852854490280153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:53<00:00,  2.51s/it]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, train loss: 3.8390228080749513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:47<00:00,  2.04s/it]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, train loss: 3.7889770793914797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:46<00:00,  1.98s/it]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, train loss: 3.8258581352233887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:50<00:00,  2.22s/it]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, train loss: 3.7717241764068605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:47<00:00,  1.99s/it]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, train loss: 3.733306703567505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:50<00:00,  2.34s/it]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, train loss: 3.6542920112609862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:51<00:00,  2.29s/it]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, train loss: 3.72977352142334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [17:35<00:00, 141.63s/it]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, train loss: 3.645867781639099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 18/50 [00:42<01:05,  2.05s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-67c7ee7b9528>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_batches_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_codes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mn_batches_per_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ferres/dev/Theano/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ferres/dev/Theano/theano/scan_module/scan_op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[1;32m    961\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[1;32m    962\u001b[0m                  allow_gc=allow_gc):\n\u001b[0;32m--> 963\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ferres/dev/Theano/theano/scan_module/scan_op.py\u001b[0m in \u001b[0;36mp\u001b[0;34m(node, args, outs)\u001b[0m\n\u001b[1;32m    950\u001b[0m                                                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m                                                 \u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m                                                 self, node)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ferres/dev/Theano/theano/scan_module/scan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/Users/ferres/.theano/compiledir_Darwin-16.7.0-x86_64-i386-64bit-i386-3.6.0b4-64/scan_perform/mod.cpp:4490)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/ferres/dev/Theano/theano/gof/op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    train_loss=0\n",
    "    for _ in tqdm(range(n_batches_per_epoch)):\n",
    "        train_loss += train_step(*generate_batch(img_codes,captions,batch_size))\n",
    "    train_loss /= n_batches_per_epoch\n",
    "    \n",
    "    \n",
    "    print('Epoch: {}, train loss: {}'.format(epoch, train_loss))\n",
    "\n",
    "print(\"Finish :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = theano.shared(np.int32(1))\n",
    "MAX_LENGTH = 20         #Change at your will"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up recurrent network that generates tokens and feeds them back to itself\n",
    "unroll_dict = dict(decoder.transition)\n",
    "unroll_dict[decoder.output_tokens] = decoder.prev_word #on next iter, output goes to input\n",
    "\n",
    "first_output = T.repeat(T.constant(START_ix,dtype='int32'),batch_size)\n",
    "init_dict = {\n",
    "    decoder.output_tokens:InputLayer([None],first_output)\n",
    "}\n",
    "\n",
    "decoder_applier = Recurrence(\n",
    "    input_nonsequences={decoder.image_features:l_image_features},\n",
    "    state_variables=unroll_dict,\n",
    "    state_init = init_dict,\n",
    "    tracked_outputs=[decoder.output_probs,decoder.output_tokens],\n",
    "    n_steps = MAX_LENGTH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "theano.config.warn_float64 = 'ignore'\n",
    "generated_tokens = get_output(decoder_applier[decoder.output_tokens])\n",
    "\n",
    "generate = theano.function([image_vectors],generated_tokens,allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-6c53291eadae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpretrained_lenet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage_to_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/Dog-and-Cat.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ferres/dev/deepbayes2017/sem3-attention/pretrained_lenet.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0mlenet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m \u001b[0mlenet_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/blvc_googlenet.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'param values'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0mset_all_param_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlenet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prob\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlenet_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ferres/.pyenv/versions/3.6.0b4/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "from pretrained_lenet import image_to_features\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "img = plt.imread(\"./data/Dog-and-Cat.jpg\")\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_to_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-687ed547f8e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput_ix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_to_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtemperature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_to_features' is not defined"
     ]
    }
   ],
   "source": [
    "output_ix = generate([image_to_features(img)])[0]\n",
    "\n",
    "for _ in range(100):\n",
    "    temperature.set_value(10)\n",
    "    print (to_string(output_ix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some tricks (for further research)\n",
    "\n",
    "* Initialize LSTM with some function of image features.\n",
    "\n",
    "* Try other attention functions\n",
    "\n",
    "* If you train large network, it is usually a good idea to make a 2-stage prediction\n",
    "    1. (large recurrent state) -> (bottleneck e.g. 256)\n",
    "    2. (bottleneck) -> (vocabulary size)\n",
    "    * this way you won't need to store/train (large_recurrent_state x vocabulary size) matrix\n",
    "    \n",
    "* Use [hierarchical softmax](https://gist.github.com/justheuristic/581853c6d6b87eae9669297c2fb1052d) or [byte pair encodings](https://github.com/rsennrich/subword-nmt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0b4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
